
# AGENTS.md â€” Projekt Scraper gier planszowych (aleplanszowki.pl)

## ğŸ¯ Cel projektu
Celem projektu jest stworzenie aplikacji umoÅ¼liwiajÄ…cej legalne, etyczne i wydajne pobieranie danych o grach planszowych z serwisu **aleplanszowki.pl**. Dane bÄ™dÄ… wykorzystywane wyÅ‚Ä…cznie w celach edukacyjnych i analitycznych, z poszanowaniem zasad robots.txt oraz dobrych praktyk scrapingowych.

---

## ğŸ§© Architektura systemu
Aplikacja skÅ‚ada siÄ™ z moduÅ‚Ã³w backendowych (scraper, baza danych, API) i frontendowych (dashboard do wizualizacji).

### Backend:
- **`scraper/`**
  - `base_scraper.py` â€“ zarzÄ…dza logikÄ… pobierania, parsowania i zapisu danych.
  - `aleplanszowki_scraper.py` â€“ specyficzna implementacja dla strony aleplanszowki.pl.
  - `selectors.py` â€“ selektory CSS / XPath dla kluczowych elementÃ³w strony (nazwa, cena, link, zdjÄ™cie, kategoria).
- **`db/`**
  - `models.py` â€“ modele SQLAlchemy (tabele: gry, kategorie, historia_cen).
  - `database.py` â€“ konfiguracja bazy SQLite i sesji ORM.
- **`api/`**
  - `main.py` â€“ FastAPI endpointy (pobieranie, filtrowanie, aktualizacja danych).
- **`utils/`**
  - `robot_check.py` â€“ sprawdzanie robots.txt (urllib.robotparser).
  - `logging_config.py` â€“ konfiguracja loggera.
  - `scheduler.py` â€“ harmonogram aktualizacji (schedule lub APScheduler).

### Frontend:
- **React / Next.js / Streamlit** â€“ do wizualizacji danych.
  - Widoki: lista gier, szczegÃ³Å‚y gry, porÃ³wnanie cen, trendy.
  - Integracja z REST API z backendu.

---

## âš™ï¸ NarzÄ™dzia i biblioteki

### Core (MVP):
```
requests
beautifulsoup4
lxml
python-dotenv
requests-cache
tenacity
SQLAlchemy
schedule
pandas
pytest
```

### Opcjonalne (rozszerzenia):
```
playwright  # dynamiczne strony JS
httpx       # async requests
selectolax  # szybki parser HTML
fake-useragent
black, isort, pre-commit  # formatowanie kodu
```

---

## ğŸ§  Zasady dziaÅ‚ania scrapera
1. **ZgodnoÅ›Ä‡ z robots.txt**  
   - Przed wykonaniem requesta, scraper uÅ¼ywa `urllib.robotparser` do weryfikacji, czy dany URL moÅ¼e byÄ‡ crawlowany.
2. **User-Agent**  
   - WysyÅ‚any w nagÅ‚Ã³wkach jako np. `"DataScienceScraperBot/1.0 (+kontakt@example.com)"`.
3. **Rate limiting i opÃ³Åºnienia**  
   - KaÅ¼de zapytanie odczekuje losowo 1â€“3 s (`time.sleep(random.uniform(1,3))`).
   - Retry/backoff na bÅ‚Ä™dach HTTP 429 i 5xx (`tenacity`).
4. **Cache HTTP**  
   - `requests-cache` przechowuje odpowiedzi lokalnie podczas developmentu.
5. **Logowanie**  
   - Wykorzystanie moduÅ‚u `logging`, poziomy INFO/ERROR.
6. **Struktura danych**  
   - KaÅ¼da gra zawiera: `id`, `tytuÅ‚`, `link`, `cena`, `dostÄ™pnoÅ›Ä‡`, `obrazek`, `kategoria`, `timestamp`.

---

## ğŸ§± Struktura projektu
```
project_root/
â”‚
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ base_scraper.py
â”‚   â”œâ”€â”€ aleplanszowki_scraper.py
â”‚   â””â”€â”€ selectors.py
â”‚
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ database.py
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ robot_check.py
â”‚   â”œâ”€â”€ logging_config.py
â”‚   â””â”€â”€ scheduler.py
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_scraper.py
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ AGENTS.md
```

---

## ğŸ§° Zadania dla agenta AI (Backend)

### `base_scraper.py`
- UtwÃ³rz klasÄ™ `BaseScraper` z metodami:
  - `fetch_html(url: str) -> str`
  - `parse_page(html: str) -> dict`
  - `save_to_db(data: dict)`
- UÅ¼yj `requests`, `BeautifulSoup` i `SQLAlchemy`.
- UwzglÄ™dnij logowanie, opÃ³Åºnienia, retry.

### `aleplanszowki_scraper.py`
- Dziedziczy po `BaseScraper`.
- W `parse_page()` znajdÅº selektory:
  - nazwa gry (`.product-title` lub `h1[itemprop='name']`),
  - cena (`.price` lub `meta[itemprop='price']`),
  - dostÄ™pnoÅ›Ä‡ (`.product-availability`),
  - obraz (`img[itemprop='image']`),
  - link (`meta[property='og:url']`).
- Zapisz dane w formacie JSON i/lub do SQLite.

### `database.py`
- Inicjalizuj bazÄ™ SQLite (`games.db`).
- Funkcja `get_session()`.

### `models.py`
- Klasy: `Game`, `Category`, `PriceHistory`.
- Pola: `id`, `name`, `url`, `price`, `availability`, `image_url`, `category_id`, `created_at`.

### `robot_check.py`
- Funkcja `is_allowed(url: str) -> bool` korzystajÄ…ca z `urllib.robotparser`.

### `scheduler.py`
- Automatyczne uruchamianie scrapera np. co 12 godzin.

---

## ğŸ’» Zadania dla agenta AI (Frontend)
- StworzyÄ‡ dashboard (React / Streamlit / Next.js) umoÅ¼liwiajÄ…cy:
  - PrzeglÄ…d gier (nazwa, cena, dostÄ™pnoÅ›Ä‡, miniaturka).
  - Filtrowanie po kategorii i przedziale cenowym.
  - Wykres historii cen (`recharts` / `plotly`).
  - Endpoint `/api/games` z backendu jako ÅºrÃ³dÅ‚o danych.

---

## âœ… Dobre praktyki
1. Nie rÃ³b zbyt wielu requestÃ³w naraz.
2. UÅ¼ywaj cache podczas testÃ³w.
3. Stosuj `.env` do danych konfiguracyjnych.
4. Utrzymuj strukturÄ™ projektu czytelnÄ… (modularnoÅ›Ä‡).
5. Testuj mockami (`pytest + responses/vcrpy`).
6. KaÅ¼dy moduÅ‚ powinien mieÄ‡ docstring i logging.
7. Wersjonuj kod w Git (branch `dev`, PR do `main`).

---

## ğŸ“… Etapy realizacji
1. Przygotowanie Å›rodowiska (Python, venv, instalacja pakietÃ³w).
2. Implementacja `BaseScraper` i `robot_check`.
3. Testy jednostkowe na pojedynczym URL.
4. Dodanie ORM i zapis danych.
5. Dodanie REST API (FastAPI).
6. Stworzenie prostego dashboardu (Streamlit lub React).
7. Dokumentacja i deployment (Docker, GitHub Actions).

---

## ğŸ“œ Licencja i etyka
Projekt ma charakter **edukacyjny**. Dane nie bÄ™dÄ… wykorzystywane komercyjnie.  
NaleÅ¼y przestrzegaÄ‡ zasad **robots.txt**, nie przeciÄ…Å¼aÄ‡ serwera oraz oznaczaÄ‡ scraper odpowiednim User-Agentem.
